{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "tPRCuE6HSZfx"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import string\n",
        "import transformers\n",
        "from pandas import DataFrame, Series\n",
        "from typing import Tuple\n",
        "from sklearn.model_selection import train_test_split\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "from tensorflow.python.keras.layers import Input, Dropout, Dense\n",
        "from tensorflow.python.keras.models import Model\n",
        "from tensorflow.python.keras.metrics import BinaryAccuracy, Precision, Recall\n",
        "\n",
        "MARKER_COLUMN_NAME = 'MARKER'\n",
        "CLASSE_COLUMN_NAME = 'DES_CLASSE'\n",
        "EMENTA_COLUMN_NAME = 'TXT_EMENTA'\n",
        "METRICS = [\n",
        "    BinaryAccuracy(name='accuracy'),\n",
        "    Precision(name='precision'),\n",
        "    Recall(name='recall')\n",
        "]\n",
        "\n",
        "\n",
        "def separate_datasets(df: DataFrame, classe: string) -> Tuple[DataFrame, DataFrame]:\n",
        "    print(\"Separando datasets\")\n",
        "    df_classe = df[df[CLASSE_COLUMN_NAME] == classe]\n",
        "    print(\"Dataset da classe-alvo:\", df_classe.shape)\n",
        "    df_outrasClasses = df[df[CLASSE_COLUMN_NAME] != classe]\n",
        "    print(\"Dataset das outras classes:\", df_outrasClasses.shape)\n",
        "    return df_classe, df_outrasClasses\n",
        "\n",
        "\n",
        "def create_downsample_df(df_classe: DataFrame, df_outrasClasses: DataFrame) -> DataFrame:\n",
        "    quantidadeDeRegistros = df_classe.shape[0]\n",
        "    print(\"Criando um dataset das outras classes com \",\n",
        "          quantidadeDeRegistros, \" registros\")\n",
        "    return df_outrasClasses.sample(quantidadeDeRegistros)\n",
        "\n",
        "\n",
        "def create_balanced_df(df_classe: DataFrame, df_outrasClasses: DataFrame) -> DataFrame:\n",
        "    df_outrasClasses_downsampled = create_downsample_df(\n",
        "        df_classe, df_outrasClasses)\n",
        "    print(\"Juntando os datasets...\")\n",
        "    df_balanced = pd.concat([df_outrasClasses_downsampled, df_classe])\n",
        "    print('Verificando a quantidade de classes existentes no DataFrame após o balanceamento:')\n",
        "    print(df_balanced[CLASSE_COLUMN_NAME].value_counts())\n",
        "    return df_balanced\n",
        "\n",
        "\n",
        "def mark_classe(df_balanced: DataFrame, classe: string):\n",
        "    print(\"Incluindo marcador na classe \", classe)\n",
        "    df_balanced[MARKER_COLUMN_NAME] = df_balanced[CLASSE_COLUMN_NAME].apply(\n",
        "        lambda x: 1 if x == classe else 0)\n",
        "    print(\"Verificando marcadores:\")\n",
        "    print(df_balanced.sample(5))\n",
        "\n",
        "\n",
        "def create_keras_model() -> Model:\n",
        "    bert_preprocess = hub.KerasLayer(AutoTokenizer.from_pretrained(\n",
        "        'neuralmind/bert-large-portuguese-cased'))\n",
        "    bert_encoder = hub.KerasLayer(AutoModel.from_pretrained(\n",
        "        'neuralmind/bert-large-portuguese-cased'))\n",
        "\n",
        "    # Bert layers\n",
        "    text_input = Input(shape=(), dtype=tf.string, name='text')\n",
        "    preprocessed_text = bert_preprocess(text_input)\n",
        "    outputs = bert_encoder(preprocessed_text)\n",
        "\n",
        "    # Neural network layers\n",
        "    l = Dropout(0.1, name=\"dropout\")(outputs['pooled_output'])\n",
        "    l = Dense(1, activation='sigmoid', name=\"output\")(l)\n",
        "\n",
        "    # Use inputs and outputs to construct a final model\n",
        "    model = Model(inputs=[text_input], outputs=[l])\n",
        "    print(model.summary)\n",
        "    return model\n",
        "\n",
        "\n",
        "def evaluate_model(model: Model, X_test: list):\n",
        "    y_predicted = model.predict(X_test)\n",
        "    y_predicted = y_predicted.flatten()\n",
        "    y_predicted = np.where(y_predicted > 0.5, 1, 0)\n",
        "    print(y_predicted)\n",
        "\n",
        "\n",
        "def train_class(df: DataFrame, classe: string,  balanceRatio=0) -> Model:\n",
        "    print('Verificando a quantidade de classes existentes no DataFrame')\n",
        "    print(df[CLASSE_COLUMN_NAME].value_counts())\n",
        "    df_classe, df_outrasClasses = separate_datasets(\n",
        "        df, classe)\n",
        "    df_balanced = create_balanced_df(df_classe, df_outrasClasses)\n",
        "    mark_classe(df_balanced, classe)\n",
        "    ementas = df_balanced[EMENTA_COLUMN_NAME]\n",
        "    markers = df_balanced[MARKER_COLUMN_NAME]\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        ementas, markers, stratify=markers)\n",
        "    model = create_keras_model()\n",
        "    model.compile(optimizer='adam',\n",
        "                  loss='binary_crossentropy',\n",
        "                  metrics=METRICS)\n",
        "    model.fit(X_train, y_train, epochs=10)\n",
        "    evaluate_model(model, X_test)\n",
        "    nomeArquivoModelo = classe + '_bert_model.h5'\n",
        "    model.save(nomeArquivoModelo)\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_leis_classificadas() -> DataFrame :\n",
        "    classes_original = pd.read_excel(\"../dados/SistemaDeClassificacao.xlsx\", \"Select viw_classificacao_arvore\")\n",
        "    classes_original.head()\n",
        "    classes_raiz = classes_original.query(\"NUM_NIVEL == 2\").filter([\"COD_CLASSE\", \"DES_NOME_PREFERIDO\"])\n",
        "    classes_raiz.rename(columns={\"DES_NOME_PREFERIDO\": \"DES_CLASSE\"}, inplace=True)\n",
        "    classes_raiz\n",
        "    leis_classificadas_original = pd.read_excel(\"../dados/ClassificacaoDeLeisOrdinarias-LeisComplementares-e-DecretosNumerados-Desde1900.xlsx\", \"Select mvw_u03_prc_doc_tema\")\n",
        "    leis_classificadas_original[\"DES_CLASSE_RAIZ\"] = leis_classificadas_original[\"DES_CLASSE_HIERARQUIA\"].apply(lambda hierarquia : hierarquia.split(\" / \")[1])\n",
        "    leis_classificadas_original.head()\n",
        "    leis_original = pd.read_excel(\"../dados/LeisOrdinarias-LeisComplementare-e-DecretosNumeradosComClassificacaoDesde1900.xlsx\", \"Select mvw_s01_documento\")\n",
        "    leis_original.rename(columns={\"DBMS_LOB.SUBSTR(S01.TXT_EMENTA\": \"TXT_EMENTA\"}, inplace=True)\n",
        "    leis_original.drop(columns=\"   \", inplace=True)\n",
        "    leis_original.head()\n",
        "    leis_original.shape[0]\n",
        "    leis = leis_original.merge(leis_classificadas_original.filter([\"COD_PROCESSO_DOCUMENTO\",\"DES_CLASSE_RAIZ\"]), left_on=\"COD_DOCUMENTO\", right_on=\"COD_PROCESSO_DOCUMENTO\", how=\"left\")\n",
        "    leis = leis.merge(classes_raiz, left_on=\"DES_CLASSE_RAIZ\", right_on=\"DES_CLASSE\", how=\"left\")\n",
        "    leis.drop(columns=[\"COD_PROCESSO_DOCUMENTO\", \"DES_CLASSE_RAIZ\"], inplace=True)\n",
        "    leis.head()\n",
        "    leis.shape[0]\n",
        "    leis.drop_duplicates(inplace=True)\n",
        "    temp = leis[[\"COD_DOCUMENTO\", \"COD_CLASSE\", \"DES_CLASSE\"]].groupby(\"COD_DOCUMENTO\")\n",
        "    temp.filter(lambda x: len(x) > 1)\n",
        "    leis.shape[0]\n",
        "    leis_classificadas = leis[leis[\"COD_CLASSE\"] >= 0]\n",
        "    return leis_classificadas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Código que efetuará o treinamento"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Verificando a quantidade de classes existentes no DataFrame\n",
            "Orçamento Público                             11192\n",
            "Infraestrutura                                 2508\n",
            "Política Social                                 877\n",
            "Administração Pública                           783\n",
            "Honorífico                                      671\n",
            "Economia e Desenvolvimento                      644\n",
            "Jurídico                                        381\n",
            "Soberania, Defesa Nacional e Ordem Pública      166\n",
            "Organização do Estado                           130\n",
            "Meio Ambiente                                    86\n",
            "Name: DES_CLASSE, dtype: int64\n",
            "Separando datasets\n",
            "Dataset da classe-alvo: (86, 6)\n",
            "Dataset das outras classes: (17352, 6)\n",
            "Criando um dataset das outras classes com  86  registros\n",
            "Juntando os datasets...\n",
            "Verificando a quantidade de classes existentes no DataFrame após o balanceamento:\n",
            "Meio Ambiente                                 86\n",
            "Orçamento Público                             47\n",
            "Infraestrutura                                18\n",
            "Economia e Desenvolvimento                     7\n",
            "Administração Pública                          6\n",
            "Política Social                                3\n",
            "Honorífico                                     2\n",
            "Soberania, Defesa Nacional e Ordem Pública     2\n",
            "Jurídico                                       1\n",
            "Name: DES_CLASSE, dtype: int64\n",
            "Incluindo marcador na classe  Meio Ambiente\n",
            "Verificando marcadores:\n",
            "       COD_DOCUMENTO               DES_NOME_PREFERIDO DES_NOMES_ALTERNATIVOS  \\\n",
            "7045          510220  Decreto nº 87.580 de 20/09/1982   DEC-87580-1982-09-20   \n",
            "33          26343755      Lei nº 13.623 de 15/01/2018   LEI-13623-2018-01-15   \n",
            "851         26339863      Lei nº 13.603 de 09/01/2018   LEI-13603-2018-01-09   \n",
            "21200       35488354  Decreto nº 10.966 de 11/02/2022   DEC-10966-2022-02-11   \n",
            "3184          501394  Decreto nº 78.754 de 18/11/1976   DEC-78754-1976-11-18   \n",
            "\n",
            "                                              TXT_EMENTA  COD_CLASSE  \\\n",
            "7045   ABRE AO MINISTERIO DA SAUDE, EM FAVOR DA SECRE...  33260515.0   \n",
            "33     Inscreve o nome de Joaquim Francisco da Costa ...  33805317.0   \n",
            "851    Altera a Lei nº 9.099, de 26 de setembro de 19...  33805362.0   \n",
            "21200  Institui o Programa de Apoio ao Desenvolviment...  33809634.0   \n",
            "3184   ABRE AO MINISTERIO DA EDUCAÇÃO E CULTURA, O CR...  33260515.0   \n",
            "\n",
            "              DES_CLASSE  MARKER  \n",
            "7045   Orçamento Público       0  \n",
            "33            Honorífico       0  \n",
            "851             Jurídico       0  \n",
            "21200      Meio Ambiente       1  \n",
            "3184   Orçamento Público       0  \n",
            "WARNING:tensorflow:Please fix your imports. Module tensorflow.python.training.tracking.data_structures has been moved to tensorflow.python.trackable.data_structures. The old module will be deleted in version 2.11.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at neuralmind/bert-large-portuguese-cased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "ename": "ValueError",
          "evalue": "Exception encountered when calling layer \"keras_layer\" \"                 f\"(type KerasLayer).\n\ntext input must of type `str` (single example), `List[str]` (batch or single pretokenized example) or `List[List[str]]` (batch of pretokenized examples).\n\nCall arguments received by layer \"keras_layer\" \"                 f\"(type KerasLayer):\n  • inputs=<KerasTensor: shape=(None,) dtype=string (created by layer 'text')>\n  • training=None",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[0;32mIn [12], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m df_leis_classificadas \u001b[38;5;241m=\u001b[39m get_leis_classificadas()\n\u001b[0;32m----> 2\u001b[0m model \u001b[38;5;241m=\u001b[39m train_class(df_leis_classificadas, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMeio Ambiente\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
            "Cell \u001b[0;32mIn [11], line 98\u001b[0m, in \u001b[0;36mtrain_class\u001b[0;34m(df, classe, balanceRatio)\u001b[0m\n\u001b[1;32m     95\u001b[0m markers \u001b[38;5;241m=\u001b[39m df_balanced[MARKER_COLUMN_NAME]\n\u001b[1;32m     96\u001b[0m X_train, X_test, y_train, y_test \u001b[38;5;241m=\u001b[39m train_test_split(\n\u001b[1;32m     97\u001b[0m     ementas, markers, stratify\u001b[38;5;241m=\u001b[39mmarkers)\n\u001b[0;32m---> 98\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_keras_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     99\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124madam\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m    100\u001b[0m               loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbinary_crossentropy\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m    101\u001b[0m               metrics\u001b[38;5;241m=\u001b[39mMETRICS)\n\u001b[1;32m    102\u001b[0m model\u001b[38;5;241m.\u001b[39mfit(X_train, y_train, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m)\n",
            "Cell \u001b[0;32mIn [11], line 67\u001b[0m, in \u001b[0;36mcreate_keras_model\u001b[0;34m()\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;66;03m# Bert layers\u001b[39;00m\n\u001b[1;32m     66\u001b[0m text_input \u001b[38;5;241m=\u001b[39m Input(shape\u001b[38;5;241m=\u001b[39m(), dtype\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mstring, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 67\u001b[0m preprocessed_text \u001b[38;5;241m=\u001b[39m \u001b[43mbert_preprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext_input\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     68\u001b[0m outputs \u001b[38;5;241m=\u001b[39m bert_encoder(preprocessed_text)\n\u001b[1;32m     70\u001b[0m \u001b[38;5;66;03m# Neural network layers\u001b[39;00m\n",
            "File \u001b[0;32m~/python_venv/wslbertenv/lib/python3.8/site-packages/keras/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[1;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
            "File \u001b[0;32m~/python_venv/wslbertenv/lib/python3.8/site-packages/tensorflow_hub/keras_layer.py:229\u001b[0m, in \u001b[0;36mKerasLayer.call\u001b[0;34m(self, inputs, training)\u001b[0m\n\u001b[1;32m    223\u001b[0m \u001b[39m# ...but we may also have to pass a Python boolean for `training`, which\u001b[39;00m\n\u001b[1;32m    224\u001b[0m \u001b[39m# is the logical \"and\" of this layer's trainability and what the surrounding\u001b[39;00m\n\u001b[1;32m    225\u001b[0m \u001b[39m# model is doing (analogous to tf.keras.layers.BatchNormalization in TF2).\u001b[39;00m\n\u001b[1;32m    226\u001b[0m \u001b[39m# For the latter, we have to look in two places: the `training` argument,\u001b[39;00m\n\u001b[1;32m    227\u001b[0m \u001b[39m# or else Keras' global `learning_phase`, which might actually be a tensor.\u001b[39;00m\n\u001b[1;32m    228\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_has_training_argument:\n\u001b[0;32m--> 229\u001b[0m   result \u001b[39m=\u001b[39m f()\n\u001b[1;32m    230\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    231\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainable:\n",
            "File \u001b[0;32m~/python_venv/wslbertenv/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2484\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.__call__\u001b[0;34m(self, text, text_pair, text_target, text_pair_target, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2482\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_in_target_context_manager:\n\u001b[1;32m   2483\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_switch_to_input_mode()\n\u001b[0;32m-> 2484\u001b[0m     encodings \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_one(text\u001b[39m=\u001b[39;49mtext, text_pair\u001b[39m=\u001b[39;49mtext_pair, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mall_kwargs)\n\u001b[1;32m   2485\u001b[0m \u001b[39mif\u001b[39;00m text_target \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   2486\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_switch_to_target_mode()\n",
            "File \u001b[0;32m~/python_venv/wslbertenv/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2542\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase._call_one\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2539\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m   2541\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m _is_valid_text_input(text):\n\u001b[0;32m-> 2542\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   2543\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mtext input must of type `str` (single example), `List[str]` (batch or single pretokenized example) \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2544\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mor `List[List[str]]` (batch of pretokenized examples).\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2545\u001b[0m     )\n\u001b[1;32m   2547\u001b[0m \u001b[39mif\u001b[39;00m text_pair \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m _is_valid_text_input(text_pair):\n\u001b[1;32m   2548\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   2549\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mtext input must of type `str` (single example), `List[str]` (batch or single pretokenized example) \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2550\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mor `List[List[str]]` (batch of pretokenized examples).\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2551\u001b[0m     )\n",
            "\u001b[0;31mValueError\u001b[0m: Exception encountered when calling layer \"keras_layer\" \"                 f\"(type KerasLayer).\n\ntext input must of type `str` (single example), `List[str]` (batch or single pretokenized example) or `List[List[str]]` (batch of pretokenized examples).\n\nCall arguments received by layer \"keras_layer\" \"                 f\"(type KerasLayer):\n  • inputs=<KerasTensor: shape=(None,) dtype=string (created by layer 'text')>\n  • training=None"
          ]
        }
      ],
      "source": [
        "df_leis_classificadas = get_leis_classificadas()\n",
        "model = train_class(df_leis_classificadas, 'Meio Ambiente')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.8.10 ('wslbertenv')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "vscode": {
      "interpreter": {
        "hash": "68f80adcc12b347753b7bc09f03dfdf8010af2edadeb24dc6c4cbd4a9e5c9d98"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
