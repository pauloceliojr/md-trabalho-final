{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "tPRCuE6HSZfx"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import string\n",
        "import transformers\n",
        "from pandas import DataFrame, Series\n",
        "from typing import Tuple\n",
        "from sklearn.model_selection import train_test_split\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "from tensorflow.python.keras.layers import Input, Dropout, Dense\n",
        "from tensorflow.python.keras.models import Model\n",
        "from tensorflow.python.keras.metrics import BinaryAccuracy, Precision, Recall\n",
        "\n",
        "MARKER_COLUMN_NAME = 'MARKER'\n",
        "CLASSE_COLUMN_NAME = 'DES_CLASSE'\n",
        "EMENTA_COLUMN_NAME = 'TXT_EMENTA'\n",
        "METRICS = [\n",
        "    BinaryAccuracy(name='accuracy'),\n",
        "    Precision(name='precision'),\n",
        "    Recall(name='recall')\n",
        "]\n",
        "\n",
        "\n",
        "def separate_datasets(df: DataFrame, classe: string) -> Tuple[DataFrame, DataFrame]:\n",
        "    print(\"Separando datasets\")\n",
        "    df_classe = df[df[CLASSE_COLUMN_NAME] == classe]\n",
        "    print(\"Dataset da classe-alvo:\", df_classe.shape)\n",
        "    df_outrasClasses = df[df[CLASSE_COLUMN_NAME] != classe]\n",
        "    print(\"Dataset das outras classes:\", df_outrasClasses.shape)\n",
        "    return df_classe, df_outrasClasses\n",
        "\n",
        "\n",
        "def create_downsample_df(df_classe: DataFrame, df_outrasClasses: DataFrame) -> DataFrame:\n",
        "    quantidadeDeRegistros = df_classe.shape[0]\n",
        "    print(\"Criando um dataset das outras classes com \",\n",
        "          quantidadeDeRegistros, \" registros\")\n",
        "    return df_outrasClasses.sample(quantidadeDeRegistros)\n",
        "\n",
        "\n",
        "def create_balanced_df(df_classe: DataFrame, df_outrasClasses: DataFrame) -> DataFrame:\n",
        "    df_outrasClasses_downsampled = create_downsample_df(\n",
        "        df_classe, df_outrasClasses)\n",
        "    print(\"Juntando os datasets...\")\n",
        "    df_balanced = pd.concat([df_outrasClasses_downsampled, df_classe])\n",
        "    print('Verificando a quantidade de classes existentes no DataFrame após o balanceamento:')\n",
        "    print(df_balanced[CLASSE_COLUMN_NAME].value_counts())\n",
        "    return df_balanced\n",
        "\n",
        "\n",
        "def mark_classe(df_balanced: DataFrame, classe: string):\n",
        "    print(\"Incluindo marcador na classe \", classe)\n",
        "    df_balanced[MARKER_COLUMN_NAME] = df_balanced[CLASSE_COLUMN_NAME].apply(\n",
        "        lambda x: 1 if x == classe else 0)\n",
        "    print(\"Verificando marcadores:\")\n",
        "    print(df_balanced.sample(5))\n",
        "\n",
        "\n",
        "def create_keras_model() -> Model:\n",
        "    bert_preprocess = AutoTokenizer.from_pretrained(\n",
        "        'neuralmind/bert-large-portuguese-cased')\n",
        "    bert_encoder = AutoModel.from_pretrained(\n",
        "        'neuralmind/bert-large-portuguese-cased')\n",
        "\n",
        "    # Bert layers\n",
        "    text_input = Input(shape=(), dtype=tf.string, name='text')\n",
        "    preprocessed_text = bert_preprocess(text_input)\n",
        "    outputs = bert_encoder(preprocessed_text)\n",
        "\n",
        "    # Neural network layers\n",
        "    l = Dropout(0.1, name=\"dropout\")(outputs['pooled_output'])\n",
        "    l = Dense(1, activation='sigmoid', name=\"output\")(l)\n",
        "\n",
        "    # Use inputs and outputs to construct a final model\n",
        "    model = Model(inputs=[text_input], outputs=[l])\n",
        "    print(model.summary)\n",
        "    return model\n",
        "\n",
        "\n",
        "def evaluate_model(model: Model, X_test: list):\n",
        "    y_predicted = model.predict(X_test)\n",
        "    y_predicted = y_predicted.flatten()\n",
        "    y_predicted = np.where(y_predicted > 0.5, 1, 0)\n",
        "    print(y_predicted)\n",
        "\n",
        "\n",
        "def train_class(df: DataFrame, classe: string,  balanceRatio=0) -> Model:\n",
        "    print('Verificando a quantidade de classes existentes no DataFrame')\n",
        "    print(df[CLASSE_COLUMN_NAME].value_counts())\n",
        "    df_classe, df_outrasClasses = separate_datasets(\n",
        "        df, classe)\n",
        "    df_balanced = create_balanced_df(df_classe, df_outrasClasses)\n",
        "    mark_classe(df_balanced, classe)\n",
        "    ementas = df_balanced[EMENTA_COLUMN_NAME]\n",
        "    markers = df_balanced[MARKER_COLUMN_NAME]\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        ementas, markers, stratify=markers)\n",
        "    model = create_keras_model()\n",
        "    model.compile(optimizer='adam',\n",
        "                  loss='binary_crossentropy',\n",
        "                  metrics=METRICS)\n",
        "    model.fit(X_train, y_train, epochs=10)\n",
        "    evaluate_model(model, X_test)\n",
        "    nomeArquivoModelo = classe + '_bert_model.h5'\n",
        "    model.save(nomeArquivoModelo)\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_leis_classificadas() -> DataFrame :\n",
        "    classes_original = pd.read_excel(\"../dados/SistemaDeClassificacao.xlsx\", \"Select viw_classificacao_arvore\")\n",
        "    classes_original.head()\n",
        "    classes_raiz = classes_original.query(\"NUM_NIVEL == 2\").filter([\"COD_CLASSE\", \"DES_NOME_PREFERIDO\"])\n",
        "    classes_raiz.rename(columns={\"DES_NOME_PREFERIDO\": \"DES_CLASSE\"}, inplace=True)\n",
        "    classes_raiz\n",
        "    leis_classificadas_original = pd.read_excel(\"../dados/ClassificacaoDeLeisOrdinarias-LeisComplementares-e-DecretosNumerados-Desde1900.xlsx\", \"Select mvw_u03_prc_doc_tema\")\n",
        "    leis_classificadas_original[\"DES_CLASSE_RAIZ\"] = leis_classificadas_original[\"DES_CLASSE_HIERARQUIA\"].apply(lambda hierarquia : hierarquia.split(\" / \")[1])\n",
        "    leis_classificadas_original.head()\n",
        "    leis_original = pd.read_excel(\"../dados/LeisOrdinarias-LeisComplementare-e-DecretosNumeradosComClassificacaoDesde1900.xlsx\", \"Select mvw_s01_documento\")\n",
        "    leis_original.rename(columns={\"DBMS_LOB.SUBSTR(S01.TXT_EMENTA\": \"TXT_EMENTA\"}, inplace=True)\n",
        "    leis_original.drop(columns=\"   \", inplace=True)\n",
        "    leis_original.head()\n",
        "    leis_original.shape[0]\n",
        "    leis = leis_original.merge(leis_classificadas_original.filter([\"COD_PROCESSO_DOCUMENTO\",\"DES_CLASSE_RAIZ\"]), left_on=\"COD_DOCUMENTO\", right_on=\"COD_PROCESSO_DOCUMENTO\", how=\"left\")\n",
        "    leis = leis.merge(classes_raiz, left_on=\"DES_CLASSE_RAIZ\", right_on=\"DES_CLASSE\", how=\"left\")\n",
        "    leis.drop(columns=[\"COD_PROCESSO_DOCUMENTO\", \"DES_CLASSE_RAIZ\"], inplace=True)\n",
        "    leis.head()\n",
        "    leis.shape[0]\n",
        "    leis.drop_duplicates(inplace=True)\n",
        "    temp = leis[[\"COD_DOCUMENTO\", \"COD_CLASSE\", \"DES_CLASSE\"]].groupby(\"COD_DOCUMENTO\")\n",
        "    temp.filter(lambda x: len(x) > 1)\n",
        "    leis.shape[0]\n",
        "    leis_classificadas = leis[leis[\"COD_CLASSE\"] >= 0]\n",
        "    return leis_classificadas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Código que efetuará o treinamento"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Verificando a quantidade de classes existentes no DataFrame\n",
            "Orçamento Público                             11192\n",
            "Infraestrutura                                 2508\n",
            "Política Social                                 877\n",
            "Administração Pública                           783\n",
            "Honorífico                                      671\n",
            "Economia e Desenvolvimento                      644\n",
            "Jurídico                                        381\n",
            "Soberania, Defesa Nacional e Ordem Pública      166\n",
            "Organização do Estado                           130\n",
            "Meio Ambiente                                    86\n",
            "Name: DES_CLASSE, dtype: int64\n",
            "Separando datasets\n",
            "Dataset da classe-alvo: (86, 6)\n",
            "Dataset das outras classes: (17352, 6)\n",
            "Criando um dataset das outras classes com  86  registros\n",
            "Juntando os datasets...\n",
            "Verificando a quantidade de classes existentes no DataFrame após o balanceamento:\n",
            "Meio Ambiente                                 86\n",
            "Orçamento Público                             57\n",
            "Infraestrutura                                16\n",
            "Administração Pública                          3\n",
            "Política Social                                3\n",
            "Jurídico                                       3\n",
            "Honorífico                                     2\n",
            "Economia e Desenvolvimento                     1\n",
            "Soberania, Defesa Nacional e Ordem Pública     1\n",
            "Name: DES_CLASSE, dtype: int64\n",
            "Incluindo marcador na classe  Meio Ambiente\n",
            "Verificando marcadores:\n",
            "       COD_DOCUMENTO               DES_NOME_PREFERIDO DES_NOMES_ALTERNATIVOS  \\\n",
            "738           488011  Decreto nº 65.372 de 13/10/1969   DEC-65372-1969-10-13   \n",
            "24779       35204048  Decreto nº 10.871 de 29/11/2021   DEC-10871-2021-11-29   \n",
            "12099         463246  Decreto nº 40.664 de 28/12/1956   DEC-40664-1956-12-28   \n",
            "12042         513545  Decreto nº 90.905 de 05/02/1985   DEC-90905-1985-02-05   \n",
            "25648       35995243  Decreto nº 11.100 de 22/06/2022   DEC-11100-2022-06-22   \n",
            "\n",
            "                                              TXT_EMENTA  COD_CLASSE  \\\n",
            "738    ABRE AO PODER JUDICIARIO - JUSTIÇA MILITAR, EM...  33260515.0   \n",
            "24779  Altera o Decreto nº 9.829, de 10 de junho de 2...  33809634.0   \n",
            "12099  ABRE AO CONSELHO DE SEGURANÇA NACIONAL O CREDI...  33260515.0   \n",
            "12042  OUTORGA CONCESSÃO A RADIO CIDADE IMPERIAL LTDA...  33805137.0   \n",
            "25648  Determina a suspensão da permissão do emprego ...  33809634.0   \n",
            "\n",
            "              DES_CLASSE  MARKER  \n",
            "738    Orçamento Público       0  \n",
            "24779      Meio Ambiente       1  \n",
            "12099  Orçamento Público       0  \n",
            "12042     Infraestrutura       0  \n",
            "25648      Meio Ambiente       1  \n"
          ]
        },
        {
          "ename": "OSError",
          "evalue": "Windows requires Developer Mode to be activated, or to run Python as an administrator, in order to create symlinks.\nIn order to activate Developer Mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "\u001b[1;32mc:\\Users\\demet\\anaconda3\\envs\\bertenv\\lib\\site-packages\\huggingface_hub\\file_download.py\u001b[0m in \u001b[0;36m_create_relative_symlink\u001b[1;34m(src, dst)\u001b[0m\n\u001b[0;32m    836\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 837\u001b[1;33m         \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msymlink\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrelative_src\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdst\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    838\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mOSError\u001b[0m: symbolic link privilege not held",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_17840\\2617071616.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mdf_leis_classificadas\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_leis_classificadas\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_class\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_leis_classificadas\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'Meio Ambiente'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_17840\\1373504164.py\u001b[0m in \u001b[0;36mtrain_class\u001b[1;34m(df, classe, balanceRatio)\u001b[0m\n\u001b[0;32m     95\u001b[0m     X_train, X_test, y_train, y_test = train_test_split(\n\u001b[0;32m     96\u001b[0m         ementas, markers, stratify=markers)\n\u001b[1;32m---> 97\u001b[1;33m     \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_keras_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     98\u001b[0m     model.compile(optimizer='adam',\n\u001b[0;32m     99\u001b[0m                   \u001b[0mloss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'binary_crossentropy'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_17840\\1373504164.py\u001b[0m in \u001b[0;36mcreate_keras_model\u001b[1;34m()\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mcreate_keras_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mModel\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m     bert_preprocess = AutoTokenizer.from_pretrained(\n\u001b[1;32m---> 60\u001b[1;33m         'neuralmind/bert-large-portuguese-cased')\n\u001b[0m\u001b[0;32m     61\u001b[0m     bert_encoder = AutoModel.from_pretrained(\n\u001b[0;32m     62\u001b[0m         'neuralmind/bert-large-portuguese-cased')\n",
            "\u001b[1;32mc:\\Users\\demet\\anaconda3\\envs\\bertenv\\lib\\site-packages\\transformers\\models\\auto\\tokenization_auto.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[0;32m    547\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    548\u001b[0m         \u001b[1;31m# Next, let's try to use the tokenizer_config file to get the tokenizer class.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 549\u001b[1;33m         \u001b[0mtokenizer_config\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_tokenizer_config\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    550\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;34m\"_commit_hash\"\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtokenizer_config\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    551\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"_commit_hash\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtokenizer_config\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"_commit_hash\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\Users\\demet\\anaconda3\\envs\\bertenv\\lib\\site-packages\\transformers\\models\\auto\\tokenization_auto.py\u001b[0m in \u001b[0;36mget_tokenizer_config\u001b[1;34m(pretrained_model_name_or_path, cache_dir, force_download, resume_download, proxies, use_auth_token, revision, local_files_only, **kwargs)\u001b[0m\n\u001b[0;32m    411\u001b[0m         \u001b[0m_raise_exceptions_for_missing_entries\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    412\u001b[0m         \u001b[0m_raise_exceptions_for_connection_errors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 413\u001b[1;33m         \u001b[0m_commit_hash\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcommit_hash\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    414\u001b[0m     )\n\u001b[0;32m    415\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mresolved_config_file\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\Users\\demet\\anaconda3\\envs\\bertenv\\lib\\site-packages\\transformers\\utils\\hub.py\u001b[0m in \u001b[0;36mcached_file\u001b[1;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, use_auth_token, revision, local_files_only, subfolder, user_agent, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash)\u001b[0m\n\u001b[0;32m    417\u001b[0m             \u001b[0mresume_download\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mresume_download\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    418\u001b[0m             \u001b[0muse_auth_token\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_auth_token\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 419\u001b[1;33m             \u001b[0mlocal_files_only\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlocal_files_only\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    420\u001b[0m         )\n\u001b[0;32m    421\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\Users\\demet\\anaconda3\\envs\\bertenv\\lib\\site-packages\\huggingface_hub\\file_download.py\u001b[0m in \u001b[0;36mhf_hub_download\u001b[1;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, user_agent, force_download, force_filename, proxies, etag_timeout, resume_download, use_auth_token, local_files_only, legacy_cache_layout)\u001b[0m\n\u001b[0;32m   1216\u001b[0m         \u001b[1;31m# we have the blob already, but not the pointer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1217\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"creating pointer to %s from %s\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mblob_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpointer_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1218\u001b[1;33m         \u001b[0m_create_relative_symlink\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mblob_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpointer_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1219\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mpointer_path\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1220\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\Users\\demet\\anaconda3\\envs\\bertenv\\lib\\site-packages\\huggingface_hub\\file_download.py\u001b[0m in \u001b[0;36m_create_relative_symlink\u001b[1;34m(src, dst)\u001b[0m\n\u001b[0;32m    840\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"nt\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    841\u001b[0m             raise OSError(\n\u001b[1;32m--> 842\u001b[1;33m                 \u001b[1;34m\"Windows requires Developer Mode to be activated, or to run Python as \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    843\u001b[0m                 \u001b[1;34m\"an administrator, in order to create symlinks.\\nIn order to \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    844\u001b[0m                 \u001b[1;34m\"activate Developer Mode, see this article: \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mOSError\u001b[0m: Windows requires Developer Mode to be activated, or to run Python as an administrator, in order to create symlinks.\nIn order to activate Developer Mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development"
          ]
        }
      ],
      "source": [
        "df_leis_classificadas = get_leis_classificadas()\n",
        "model = train_class(df_leis_classificadas, 'Meio Ambiente')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.7.13 ('bertenv')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "383ce97f92d092bf4e45ee69a69d23ab3aca885d6ccc5a190fa970b631a7a0a8"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
